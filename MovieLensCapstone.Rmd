---
title: "MovieLens project"
author: "Kjersti Framnes"
date: "2023-08-09"
output: pdf_document
---



## Introduction to project

As part of the HarvardX PH125.9x Data Science: Capstone course, I've undertaken a project involving the MovieLens dataset. In this report, I'll outline the project's key goals, approach, and my findings.

Recommendation systems leverage user ratings to generate customized item suggestions for users. These systems are beneficial to companies like Amazon, who collect vast amounts of data from users rating their products. By predicting a user's potential rating for an item, these systems can make targeted recommendations that align with a user's interests and preferences. This methodology can also be applied to movie recommendations, which is the focus of this project. Recommendation systems are a staple in machine learning, as evidenced by the success of Netflix's powerful recommendation engine. The Netflix Prize, a competition to develop the best algorithm for predicting user ratings for films based solely on past ratings, underscores the importance of recommendation algorithms in product recommendations.

### Project Objectives

The primary goal of this project is to design and train a machine learning algorithm capable of predicting user movie ratings on a scale of 0.5 to 5 stars. I will use a subset of the data (the edx dataset provided by the course staff) to train the algorithm and evaluate its performance on a validation set.

I will assess the algorithm's performance using the Root Mean Square Error (RMSE). RMSE is a commonly used metric for evaluating the accuracy of predictive models by measuring the differences between the observed values and the values predicted by the model. A lower RMSE indicates better model performance. RMSE is calculated by taking the square root of the average of the squared differences between the observed and predicted values. Since each error's impact on the RMSE is proportional to the size of the squared error, RMSE is sensitive to outliers and larger errors have a disproportionately large effect.

In this project, I will develop four models and compare their performance using the RMSE.

`\newpage

## Set up: 

```{r setup,echo = TRUE,warning=FALSE,message=FALSE}
#########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(gt)) install.packages("gt", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(ggpubr)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")



library(tidyverse)
library(caret)
library(gt)
library(ggplot2)
library(tidyr)
library(scales)
library(knitr)
library(ggpubr)
library(gridExtra)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
knitr::opts_chunk$set(echo = FALSE,warning=FALSE)
```
`\newpage

## Create a train and test set:

After creating the validation set called "final_holdout_test" we need to partition further to obtain a train and test dataset. When I have experimented with different models to obtain the lowest RMSE I will run the same models using the unseen validation dataset ("final_holdout_test). This in order to avoid over fitting our model and verifying the best model. 

```{r echo = TRUE,warning=FALSE,message=FALSE}
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
edx_train <- edx[-test_index,]
edx_temp <- edx[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
edx_test <- edx_temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(edx_temp, edx_test)
edx_train <- rbind(edx_train, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed, edx_temp)

```

`\newpage

## Exploratory analyis of data

Information about the dataset:

```{r edxSummary,out.width="80%"}
str(edx)
```

Summary of each column in the dataset: 

```{r edxSummary2,out.width="80%"}
summary(edx)
```
`\newpage 

### Lets see how the ratings looks like:  

```{r edxRatings, echo=FALSE,out.width="60%", fig.align='left'}
# Load the ggplot2 library for data visualization
library(ggplot2)

# Plotting a histogram for the 'rating' column of the 'edx' dataset
ggplot(edx, aes(x=rating)) +  # Start the plot specifying 'rating' as the x-variable

  # Add a histogram to the plot with specified design details
  geom_histogram(binwidth=0.5, fill="blue", color="white", alpha=1) +

  # Annotate each bar in the histogram with its count (frequency) value
  stat_count(aes(x = rating, y = ..count.., label = format(..count.., big.mark = " ")), 
             geom = "text", position = position_nudge(y = 30), vjust = -0.5) +

  # Define the title and axis labels of the plot
  labs(title="Distribution of Ratings", x="Rating", y="Frequency") +

  # Apply a minimal theme to the plot
  theme_minimal() +

  # Further customize the plot theme to remove certain elements and grids
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +

  # Define x-axis breaks and remove expansion around the data in the x-axis
  scale_x_continuous(breaks = seq(min(edx$rating, na.rm = TRUE), max(edx$rating, na.rm = TRUE), by = 0.5), 
                     expand = c(0, 0))


```
We see that three ratings are more populare in the dataset. 



Lets have a look at the different genres in the dataset. Important to note that a movie can have more than one genre. But if I split the genres into single genres we can see that the genre with the most rating is "Drama". And looking at the averge rating for each single genre we see that the genre "Film Noir" (low number of ratings) has the highest average rating and the genre "Horror" has the lowest average rating. 

```{r edxGenres, echo=FALSE, message=FALSE, out.width="80%", fig.align='left'}


# Summarizing both the rating count and average rating
result <- edx %>%
  separate_rows(genres, sep="\\|") %>%
  group_by(genres) %>%
  summarise(average_rating = mean(rating, na.rm = TRUE),
            rating_count = n())

# Sorting based on rating_count
sorted_result <- result %>%
  arrange(desc(rating_count))

# Plot for Rating Counts by Genre
p1 <- ggplot(sorted_result, aes(x = reorder(genres, rating_count), y = rating_count)) +
  geom_bar(stat = "identity", fill = "blue") +
  geom_text(aes(label = comma(rating_count, big.mark = " ")), hjust = -0.1, color = "black", size = 3) + 
  coord_flip() +
  labs(title = "Rating Counts by Genre", x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), 
        axis.ticks.x = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, max(sorted_result$rating_count) + 0.11 * max(sorted_result$rating_count)), expand = c(0, 0))

print(p1)

# Plot for Average Rating by Genre
p2 <- ggplot(sorted_result, aes(x = reorder(genres, rating_count), y = average_rating)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = sprintf("%.2f", average_rating)), hjust = -0.1, color = "black", size = 3) + 
  coord_flip() +
  labs(title = "Average Rating by Genre (sorted by Rating Count)", x = "", y = "Average Rating") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

print(p2)

```


## Most rated movies: 

```{r edxtitles, echo=FALSE, message=FALSE, out.width="80%", fig.align='left'}

# Using the 'edx' dataset to find the top ten most rated movies
top_ten_most_rated_movies <- edx %>%

  # Group data by the 'title' column
  group_by(title) %>%

  # Summarize each group to calculate the number of ratings and the average rating
  summarise(
    rating_count = n(),  # Count the number of ratings for each movie
    average_rating = mean(rating, na.rm = TRUE)  # Calculate the average rating for each movie, ignoring NA values
  ) %>%

  # Sort the summarized data in descending order based on the rating count
  arrange(desc(rating_count)) %>%

  # Select the top 10 movies from the sorted list
  head(10)

# Display the top ten most rated movies using 'kable' for a neat tabular format with a caption
kable(top_ten_most_rated_movies, caption = "Top 10 Most Rated Movies with Average Ratings")

```

In this graph we can clearly see what rating are the most populare; 

```{r edxrank, echo=FALSE, message=FALSE, out.width="80%", fig.align='left'}

# Group data by rating, count frequency, and rank
rating_rank <- edx %>%
  group_by(rating) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  mutate(rank = row_number())

# Custom label function
comma_format <- function(x) {
  format(x, big.mark = " ")
}

# Create a bar chart
ggplot(rating_rank, aes(x = reorder(rating, freq), y = freq)) +
  geom_bar(stat = "identity", fill = "blue") +
  geom_text(aes(label = comma_format(freq)), hjust = 1, color = "white") +  # Use the custom label function
  coord_flip() +
  labs(title = "Frequency of Ratings Ranked", x = "Rating", y = "Frequency") +
  theme_minimal()

```

If we look at ratings given as whole numbers or half numbers we clearly see that the critics are more fab of giving ratings as whole numbers. Maybe this means they are more certain when they give ratings. 

```{r edxratingsWhole, echo=FALSE, message=FALSE, out.width="50%", fig.align='left'}


# Creating a summary of the ratings based on whether they are whole numbers or not
rating_summary <- edx %>%

  # Add a new column 'is_whole_number' which indicates if a rating is a whole number or not
  mutate(is_whole_number = ifelse(rating == floor(rating), "Whole Number", "Not Whole Number")) %>%

  # Group data by the 'is_whole_number' column
  group_by(is_whole_number) %>%

  # Summarize each group to calculate the count of ratings
  summarise(count = n())

# Visualizing the distribution of whole number ratings versus non-whole number ratings using a pie chart
ggplot(rating_summary, aes(x = "", y = count, fill = is_whole_number)) +  # Setting up the plot with empty x-axis and 'count' as y-axis

  # Add bars to the plot, using the 'count' values to determine bar height
  geom_bar(stat = "identity", width = 1) +

  # Convert the bar chart to a pie chart
  coord_polar(theta = "y") +

  # Define the title and legend labels of the plot
  labs(title = "Distribution of Ratings: Whole vs. Not Whole Number", fill = "Rating Type") +

  # Apply a minimal theme to the plot
  theme_minimal() +

  # Further customize the plot theme to remove certain elements and grids for cleaner pie chart visualization
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.grid = element_blank(),
        panel.border = element_blank())



```

Now I also wanted to look at the ratings given over years. Here I had to convert the timestamp into a more workable format; as.Date(as.POSIXct(edx$timestamp, origin="1970-01-01", tz="UTC"))

By looking at the time series grapg we can see that in the years 1996, 2000 and 2005 most ratings were given. 

```{r edxratingsovertime, echo=FALSE, message=FALSE, out.width="80%", fig.align='left'}

# Convert the 'timestamp' column from the 'edx' dataset into a date format
edx$date <- as.Date(as.POSIXct(edx$timestamp, origin="1970-01-01", tz="UTC"))
# The 'timestamp' is assumed to be in a Unix timestamp format. 
# We're converting it to a POSIXct object using an origin of "1970-01-01" (standard for Unix time) and then into a Date object.

# Extract the year from the newly created 'date' column and store it in a new column 'year'
edx$year <- year(edx$date)

# Extract the month as a number from the 'date' column and store it in a new column 'month_number'
edx$month_number <- month(edx$date)

# Extract the month as text (e.g., "Jan" for January) from the 'date' column and store it in a new column 'month_text'
edx$month_text <- month(edx$date, label = TRUE)
# The 'label = TRUE' argument ensures that the month is returned as a textual abbreviation.


# Adjusting the date and extracting the year
edx$date <- as.Date(as.POSIXct(edx$timestamp, origin="1970-01-01", tz="UTC"))
edx$year <- year(edx$date)

# Grouping by year and summarizing
yearly_ratings <- edx %>%
  group_by(year) %>%
  summarise(rating_count = n())

# Creating the plot with ggplot2
p <- ggplot(yearly_ratings, aes(x = as.factor(year), y = rating_count)) +
  geom_line(aes(group = 1), color = "blue", size = 1) +  # Group = 1 ensures the line connects across years
  geom_point(color = "red", size = 1.5) + 
  labs(title = "Number of Ratings per Year", x = "Year", y = "Number of Ratings") +
  theme_minimal() + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10), 
                     labels = function(x) format(x, big.mark = " ")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Display the plot
print(p)


```


`\newpage

## Modelling

Introduction: 
We need to build a function that we will use with the different models: 

```{r Model_build, include=TRUE,echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
# Define a function 'RMSE' to calculate the Root Mean Square Error between true ratings and predicted ratings.
RMSE <- function(true_ratings, predicted_ratings) {
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```


### Model 1: Average rating 

Firstly we need to calculate the mean to have a benchmark; 

```{r Model1_1, include=FALSE,echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
mu <- mean(edx_train$rating)
mu
```

```{r Model1_2,echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
#Creating the first model
Model1_rmse <- RMSE(edx_test$rating,mu)
model_results <- tibble(Model = "1. Average rating model", RMSE = Model1_rmse)

```

```{r Model1_2_table,echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}

#Creating the table containing the model results
formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE"
  ) %>%
  fmt_number(
    columns = c("RMSE"),
    decimals = 4  # Format RMSE with 2 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )

formatted_table

```


### Model 2: Movie effect

The inherent quality of movies can cause variations in their ratings. To account for this, we introduce a "bias" term, b_i, to our model, representing the average deviation of ratings for a specific movie from the overall average rating (\( \mu \)). While these deviations are traditionally called "effects" in statistics, they're termed "bias" in contexts like the Netflix challenge. Instead of using the computationally intensive lm() function to estimate these bias terms for each movie, we can directly compute them by subtracting the overall average rating from the average rating of each movie. This approach offers a more efficient way to capture the inherent differences in movie ratings.


```{r Model2, echo=FALSE, message=FALSE, out.width="80%", fig.align='left'}

# Calculate the average bias (b_i) for each movie from the 'edx' dataset
avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

# Create a histogram plot of the calculated b_i values
hist_plot <- ggplot(avgs, aes(x = b_i)) + 

  # Add bars to the histogram
  geom_histogram(aes(y = ..density..), bins = 10, fill = "blue", color = "black") +

  # Add a density curve to the histogram
  geom_density(alpha = .2, fill = "#FF6666") +

  # Set the title and axis labels for the plot
  labs(
    title = "Histogram of b_i Values",
    x = "b_i Value",
    y = "Density"
  ) +

  # Apply a minimal theme to the plot
  theme_minimal() +

  # Adjust the x-axis text for better readability
  theme(
    axis.text.x = element_text(angle = 0, hjust = 1)
  )

# Print the finalized histogram plot
print(hist_plot)

```


```{r Model2_1, echo=TRUE, message=FALSE, out.width="80%", fig.align='left'}

# Predict ratings using a baseline and bias for each movie
prediction <- mu + edx_test %>%
  left_join(avgs, by='movieId') %>%  # Join 'edx_test' with 'avgs' on 'movieId'
  pull(b_i)                          # Extract bias values

# Calculate the RMSE between predicted ratings and actual ratings from 'edx_test'
Model2_rmse <- RMSE(prediction, edx_test$rating)

```

```{r Model2_1_table, echo=FALSE, message=FALSE, out.width="80%", fig.align='left'}


new_row <- tibble(Model = "2. Movie effect model", RMSE = Model2_rmse)

# Remove the existing row for "2. Movie effect model"
model_results <- model_results %>%
  filter(Model != "2. Movie effect model")

# Append the new row
model_results <- bind_rows(model_results, new_row)

library(gt)

formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE"
  ) %>%
  fmt_number(
    columns = c("RMSE"),
    decimals = 4  # Format RMSE with 2 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )

formatted_table
```


### Model 3: Movie and user effect

We want to reduce RSME further. 
While movies have their inherent appeal or lack thereof, users also bring their unique biases to the table. By acknowledging both these aspects, the model aims to make more accurate predictions on how a user might rate a particular movie.

Looking at this we can see that there are some extreme values. Although user avg rating seems to be close to normally distributed we can see from the historgams and the Q-Q plot that we have longer negative / left side tail indicating there are more extreme negative values and we can not say the user avg rating is normally distrubuted. 

```{r Model3_1, echo=FALSE, message=FALSE, out.width="100%", fig.align='left',fig.ncol=2, fig.layout="matrix"}

# Filter users who have rated 100 or more movies and compute their average rating
user_avg_rating <- edx %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(average_rating = mean(rating))

# Plot the scatterplot of users' average ratings
scatter_plot <- ggplot(user_avg_rating, aes(x = userId, y = average_rating)) +
  geom_point(aes(color = average_rating), size = 3) + 
  geom_hline(aes(yintercept = mean(average_rating)), color = "red", linetype="dashed") +
  labs(title = "Avg Rating Users (100+ Ratings)", x = "User ID", y = "Avg Rating") +
  theme_minimal() +
  scale_color_gradient(low="blue", high="red") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

# Plot histogram
histogram_plot <- ggplot(user_avg_rating, aes(average_rating)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(
    title = "Avg Ratings Users(100+ movies)", 
    y = "Number of Users",
    x = "Avg Rating"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 10))

# Histogram with density curve
density_plot <- ggplot(user_avg_rating, aes(x=average_rating)) +
  geom_histogram(aes(y=..density..), binwidth=0.1, fill="blue", color="black", alpha=0.7) +
  geom_density(color="red", lwd=1) +
  labs(title="Histogram of Avg Ratings w/ DC", 
       x="Avg Rating", 
       y="Density") +
  theme_minimal()

# Q-Q Plot
qq_plot <- ggqqplot(user_avg_rating$average_rating, 
         ylab = "Average Ratings", 
         xlab = "Theoretical Quantiles") +
  labs(title="Q-Q Plot of Avg Ratings") +
  theme_minimal()

# Displaying plots in a 2x2 grid
library(gridExtra)
grid.arrange(scatter_plot, histogram_plot, density_plot, qq_plot, ncol=2)


```


The User-Movie Rating Model predicts a user's movie rating based on two main factors:

Movie Effect (b_i): Reflects the inherent quality or appeal of a movie.
User Effect (b_u): Represents an individual user's tendency to rate movies more or less favorably.
The mathematical representation of the model is:

The equation \( y_{u,i} = \mu + b_i + b_u + \epsilon_{u,i} \) 

\( y_{u,i} \) is the predicted rating of user \( u \) for movie \( i \).

\( \mu \) denotes the average rating across all movies.

\( b_i \) is the movie effect.

\( b_u \) is the user effect.

\( \epsilon_{u,i} \) represents random errors centered at 0.


In essence, the model combines the overall movie appeal and the individual biases of users to forecast ratings. By incorporating both these elements, it aims to achieve more precise predictions on how a specific user might rate a given movie.


```{r Model3_2, echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}

# Calculate the average rating bias for each user in the 'edx_train' dataset
user_avg_rating_edx_train <- edx_train %>% 
  left_join(avgs, by='movieId') %>%        # Join with 'avgs' to get movie biases (b_i)
  group_by(userId) %>%                     # Group by user
  summarize(b_u=mean(rating - mu - b_i))   # Calculate user bias (b_u) for each user

# Predict ratings for 'edx_test' using global average, movie bias, and user bias
prediction_model3 <- edx_test %>% 
  left_join(avgs, by='movieId') %>%                           # Join with 'avgs' to get movie biases (b_i)
  left_join(user_avg_rating_edx_train, by='userId') %>%       # Join with user biases (b_u)
  mutate(pred_m3 = mu + b_i + b_u) %>%                        # Compute the predicted ratings
  pull(pred_m3)                                               # Extract the predicted values

# Calculate the RMSE between predicted ratings and actual ratings from 'edx_test' for Model 3
model3_rmse <-  RMSE(prediction_model3, edx_test$rating)

```


```{r Model3_2_table, echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}


# Create a new row
new_row <- tibble(Model = "3. Movie and user effect model", RMSE = model3_rmse)

# Remove the existing row for "3. Movie and user effect model"
model_results <- model_results %>%
  filter(Model != "3. Movie and user effect model")


# Append the new row
model_results <- bind_rows(model_results, new_row)

# Display the formatted table using gt
library(gt)

formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE"
  ) %>%
  fmt_number(
    columns = c("RMSE"),
    decimals = 4  # Format RMSE with 4 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )

formatted_table



```



In our initial model, we found that movies with few ratings could have extreme bias terms, \( b_i \) , leading to higher RMSE in our predictions. We previously used confidence intervals to address this uncertainty, but for predictions, we need a single estimate.
Regularization offers a solution by adding a penalty for large \( b_i \) values in the minimization process, reducing the influence of extreme bias estimates from small sample sizes. The regularization parameter, \( \lambda \) determines the penalty strength. We must find the optimal \( \lambda \)  to minimize the RMSE. Regularization shrinks the \( b_i \)  and \( b_u \) terms, especially for small rating counts, mitigating outliers' impact on our predictions.


### Model 4: Regularized movie and user effect

```{r Model4, echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
# Initialize a sequence of lambda values from 0 to 10 in increments of 0.1
lambdas <- seq(0, 10, 0.25)

# Use sapply to calculate the RMSE for each lambda value
RMSES <- sapply(lambdas, function(l){
  # Calculate the global mean rating from the edx_train dataset
  edx_train_mu <- mean(edx_train$rating)
  
  # Calculate the bias term b_i for each movie in the edx_train dataset
  b_i <- edx_train %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - edx_train_mu)/(n() + l))
  
  # Calculate the bias term b_u for each user in the edx_train dataset
  b_u <- edx_train %>%
    left_join(b_i, by='movieId') %>% 
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - edx_train_mu)/(n() +l))
  
  # Predict the ratings for the edx_test dataset based on the bias terms b_i and b_u
  predicted_ratings <- edx_test %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = edx_train_mu + b_i +  b_u) %>% .$pred
  
  # Return the RMSE between the predicted ratings and the actual ratings in the edx_test dataset
  return(RMSE(predicted_ratings, edx_test$rating))
})

# Find the lambda value that gives the lowest RMSE
lambda <- lambdas[which.min(RMSES)]
lambda



```
The best lambda is `r lambda`. 



If we look at the plot we can verify the optimal lambda: 

```{r Model4_plot, echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}

qplot(lambdas,RMSES)
```

```{r Model4_model, echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
edx_train_mu <- mean(edx_train$rating)

# Calculate the bias term b_i for each movie in the edx_train dataset
b_i <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - edx_train_mu)/(n()+lambda))

# Calculate the bias term b_u for each user in the edx_train dataset
b_u <-edx_train %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - edx_train_mu)/(n()+lambda))

# Predict the ratings for the edx_test dataset based on the bias terms b_i and b_u
prediction_model4 <- edx_test %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(predictions = edx_train_mu + b_i + b_u) %>% .$predictions

model4_rmse <-RMSE(prediction_model4,edx_test$rating)

```


```{r ModelSummary, echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}
# Create a new row for model 4
new_row_model4 <- tibble(Model = "4. (Regularized) Movie and user effect model", RMSE = model4_rmse)

# Append the new row for model 4
model_results <- bind_rows(model_results, new_row_model4)

# Display the formatted table using gt
library(gt)

formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE"
  ) %>%
  fmt_number(
    columns = c("RMSE"),
    decimals = 4  # Format RMSE with 4 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )

formatted_table


```

`\newpage

## Summmary: 

### Average Rating Model (RMSE: 1.0601)
This is a basic model that predicts the average rating for all movies. Its relatively high RMSE indicates that it doesn't account for variations in individual movie ratings or user preferences, and therefore may not be very accurate.
### Movie Effect Model (RMSE: 0.9417)
This model introduces a factor for individual movies, presumably improving its accuracy compared to the simple average rating model. The RMSE has decreased, suggesting better performance in predicting movie ratings.
### Movie and User Effect Model (RMSE: 0.8636)
This model incorporates effects from both movies and users. This means it accounts for variations in ratings by movie as well as variations in ratings by individual users. Its lower RMSE suggests that taking both movie and user effects into consideration leads to more accurate predictions.
### (Regularized) Movie and User Effect Model (RMSE: 0.8641)
This model is a variation of the previous one but includes regularization, which is a technique used to avoid overfitting. The RMSE is slightly higher than the non-regularized version. This could indicate that while the model may generalize better to unseen data (a typical advantage of regularization), it might not perform as well on the training data or the specific test data provided.

In summary, as we move from the first to the third model, the RMSE decreases, indicating improvements in prediction accuracy. The fourth model, while having a slightly higher RMSE than the third, might be better suited for new or unseen data due to its regularization.

`\newpage

## Now we need to run all the models using the validation ("final_holdout_test") dataset. 

### Model 1: Average rating 

Firstly we need to calculate the mean; 

```{r Model1_1_final, include=FALSE,echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
mu_final <- mean(edx$rating)
mu_final
```

```{r Model1_2_final,echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
#Creating the first model
Model1_rmse_final <- RMSE(final_holdout_test$rating,mu_final)

```

```{r Model1_2_table_final,echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}
#Creating table
model_results <- tibble(
  Model = c("1. Average rating model"),
  RMSE = c(Model1_rmse) 
)


Model1_rmse_final_value <- Model1_rmse_final 



model_results <- model_results %>%
  mutate(RMSE_Final = case_when(
    Model == "1. Average rating model" ~ Model1_rmse_final_value,
    TRUE ~ NA_real_
  ))


formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE",
    RMSE_Final = "RMSE Final"
  ) %>%
  fmt_number(
    columns = c("RMSE", "RMSE_Final"),
    decimals = 4  # Format RMSE with 2 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )


formatted_table



```

### Model 2: Movie effect with validation data
```{r Model2_1_final, echo=TRUE, message=FALSE, out.width="80%", fig.align='left'}
# Calculate the average bias (b_i_final) for each movie from the 'edx' dataset
avgs_final <- edx %>%
  group_by(movieId) %>%
  summarize(b_i_final = mean(rating - mu_final))  # Compute bias by subtracting global average (mu_final)

# Predict ratings for 'final_holdout_test' using the global average (mu_final) and movie bias (b_i_final)
prediction_final <- mu_final + final_holdout_test %>%
  left_join(avgs_final, by='movieId') %>%         # Join with 'avgs_final' to get the movie biases
  pull(b_i_final)                                 # Extract the movie bias values

# Calculate the RMSE between the predicted ratings and actual ratings from 'final_holdout_test'
Model2_rmse_final <- RMSE(prediction_final, final_holdout_test$rating)


```

```{r Model2_1_final_table,echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}
#Creating table
model_results <- tibble(
  Model = c("1. Average rating model", "2. Movie effect model"),
  RMSE = c(Model1_rmse, Model2_rmse) 
)


Model1_rmse_final_value <- Model1_rmse_final 
Model2_rmse_final_value <- Model2_rmse_final 


model_results <- model_results %>%
  mutate(RMSE_Final = case_when(
    Model == "1. Average rating model" ~ Model1_rmse_final_value,
    Model == "2. Movie effect model" ~ Model2_rmse_final_value,
    TRUE ~ NA_real_
  ))


formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE",
    RMSE_Final = "RMSE Final"
  ) %>%
  fmt_number(
    columns = c("RMSE", "RMSE_Final"),
    decimals = 4  # Format RMSE with 2 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )


formatted_table


```



### Model 3: Movie and user effect with validation data


```{r Model3_2_final, echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
# Calculate the user-specific bias (b_u_final) from the 'edx' dataset
user_avg_rating_edx <- edx %>% 
  left_join(avgs_final, by='movieId') %>%          # Join with 'avgs_final' to get movie biases (b_i_final)
  group_by(userId) %>%                             # Group by user
  summarize(b_u_final=mean(rating - mu_final - b_i_final))   # Compute user bias by subtracting global average and movie bias

# Predict ratings for 'final_holdout_test' using global average (mu_final), movie bias (b_i_final), and user bias (b_u_final)
prediction_model3_final <- final_holdout_test %>% 
  left_join(avgs_final, by='movieId') %>%                          # Join with 'avgs_final' to get movie biases
  left_join(user_avg_rating_edx, by='userId') %>%                  # Join with user biases
  mutate(pred_m3_final = mu_final + b_i_final + b_u_final) %>%     # Compute the predicted ratings
  pull(pred_m3_final)                                              # Extract the predicted values

# Calculate the RMSE for Model 3 between predicted and actual ratings from 'final_holdout_test'
model3_rmse_final <-  RMSE(prediction_model3_final, final_holdout_test$rating)
model3_rmse_final  # Output the RMSE value

```

```{r Model3_1_final_table,echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}


model_results <- tibble(
  Model = c("1. Average rating model", "2. Movie effect model","3. Movie and user effect model"),
  RMSE = c(Model1_rmse, Model2_rmse,model3_rmse) 
)

Model1_rmse_final_value <- Model1_rmse_final 
Model2_rmse_final_value <- Model2_rmse_final 
Model3_rmse_final_value <- model3_rmse_final

model_results <- model_results %>%
  mutate(RMSE_Final = case_when(
    Model == "1. Average rating model" ~ Model1_rmse_final_value,
    Model == "2. Movie effect model" ~ Model2_rmse_final_value,
    Model == "3. Movie and user effect model" ~ Model3_rmse_final_value,
    TRUE ~ NA_real_
  ))

formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE",
    RMSE_Final = "RMSE Final"
  ) %>%
  fmt_number(
    columns = c("RMSE", "RMSE_Final"),
    decimals = 4  # Format RMSE with 2 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )


formatted_table


```


### Model 4: Regularized movie and user effect with validation data

```{r Model4_model_final, echo=TRUE, message=FALSE, out.width="100%", fig.align='left'}
edx_mu <- mean(edx$rating)

# Calculate the bias term b_i for each movie in the edx_train dataset
b_i_final <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i_final = sum(rating - edx_mu)/(n()+lambda))

# Calculate the bias term b_u for each user in the edx_train dataset
b_u_final <-edx %>% 
  left_join(b_i_final, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_final = sum(rating - b_i_final - edx_mu)/(n()+lambda))

# Predict the ratings for the edx_test dataset based on the bias terms b_i and b_u
prediction_model4_final <- final_holdout_test %>% 
  left_join(b_i_final, by = "movieId") %>%
  left_join(b_u_final, by = "userId") %>%
  mutate(predictions = edx_mu + b_i_final + b_u_final) %>% .$predictions

model4_rmse_final <-RMSE(prediction_model4_final,final_holdout_test$rating)

```


```{r Model4_1_final_table,echo=FALSE, message=FALSE, out.width="100%", fig.align='left'}


model_results <- tibble(
  Model = c("1. Average rating model", "2. Movie effect model","3. Movie and user effect model","4. (Regularized) Movie and user effect model"),
  RMSE = c(Model1_rmse, Model2_rmse,model3_rmse,model4_rmse) 
)

Model1_rmse_final_value <- Model1_rmse_final 
Model2_rmse_final_value <- Model2_rmse_final 
Model3_rmse_final_value <- model3_rmse_final
Model4_rmse_final_value <- model4_rmse_final

model_results <- model_results %>%
  mutate(RMSE_Final = case_when(
    Model == "1. Average rating model" ~ Model1_rmse_final_value,
    Model == "2. Movie effect model" ~ Model2_rmse_final_value,
    Model == "3. Movie and user effect model" ~ Model3_rmse_final_value,
    Model == "4. (Regularized) Movie and user effect model" ~ Model4_rmse_final_value,
    TRUE ~ NA_real_
  ))


formatted_table <- model_results %>%
  gt() %>%
  tab_header(
    title = "Model Results",
    subtitle = "Performance Metrics"
  ) %>%
  cols_label(
    Model = "Model Name",
    RMSE = "RMSE",
    RMSE_Final = "RMSE Final"
  ) %>%
  fmt_number(
    columns = c("RMSE", "RMSE_Final"),
    decimals = 4  # Format RMSE with 2 decimal places
  ) %>%
  tab_options(
    heading.title.font.size = 20,
    heading.subtitle.font.size = 16
  ) %>%
  tab_style(
    style = list(
      cell_text(weight = "bold")
    ),
    locations = cells_column_labels(columns = everything())
  )


formatted_table


```


## Summary and Conclusion:
  
In our evaluation of the models against the unseen dataset, we noticed a clear pattern of improved predictive accuracy. Starting from the Average Rating Model and moving to the Regularized Movie and User Effect Model, there's a consistent decrease in RMSE. This shows the benefits of adding more detailed features and regularization techniques.

However, an important observation to make is that each model displayed a slightly higher RMSE when tested on the unseen data compared to results from our train/test split. This indicates that, while our models are improving, they still face a slight challenge when adapting to completely new data.

n our pursuit to refine our predictive models, we've achieved significant improvements. However, by incorporating factors such as the movie's release date (indicating its age) and main genre, there's potential for further optimization, ensuring more consistent performance across varied datasets.

There are more modeling methods we can consider to potentially boost our predictions. We could look into deep learning, which, while powerful, can be resource-intensive and might require more than a standard Mac to run efficiently. Techniques like matrix factorization, specifically Singular Value Decomposition (SVD), could be applied â€“ they've been popular for recommendation tasks. Ensemble methods, where we combine different models, are another avenue. However, these can sometimes be computationally demanding as well.

